{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    request_timeout=300.0,\n",
    "    additional_kwargs={\"num_ctx\": 16384, \"num_predict\": -1},\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "# response = llm.stream_complete(\"What are the key findings of llama2 paper?\")\n",
    "\n",
    "# for r in response:\n",
    "#     print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:27<00:00,  5.41s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "\n",
    "embed_model = FastEmbedEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "[-0.0033337711356580257, -0.011862986721098423, 0.04161721095442772, -0.03819842264056206, 0.024203987792134285]\n"
     ]
    }
   ],
   "source": [
    "embeddings = embed_model.get_text_embedding(\"Hello World!\")\n",
    "print(len(embeddings))\n",
    "print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PyMuPDFReader.load_data() got an unexpected keyword argument 'show_progress'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyMuPDFReader\n\u001b[0;32m----> 5\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mPyMuPDFReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/llama2.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# loader = PyMuPDFReader()\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# documents = loader.load(file_path=\"./data/llama2.pdf\")\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: PyMuPDFReader.load_data() got an unexpected keyword argument 'show_progress'"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "documents = PyMuPDFReader().load_data(file_path=\"./data/llama2.pdf\")\n",
    "# loader = PyMuPDFReader()\n",
    "# documents = loader.load(file_path=\"./data/llama2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 77/77 [00:00<00:00, 2135.34it/s]\n",
      "Generating embeddings: 100%|██████████| 77/77 [00:07<00:00,  9.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize vector database and add nodes to it\n",
    "\n",
    "from llama_index.core import Settings, StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "import qdrant_client\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "    # location=\":memory:\",\n",
    "    host=\"localhost\",\n",
    "    port=6333,\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"rag_demo_collection\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=3000, chunk_overlap=400),\n",
    "        ],\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "# configure retriever\n",
    "# retriever = VectorIndexRetriever(\n",
    "#     index=index,\n",
    "#     similarity_top_k=5,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_str = \"What are the difference variants of llama 2 model?\"\n",
    "\n",
    "# response_nodes = retriever.retrieve(query_str)\n",
    "\n",
    "# for node in response_nodes:\n",
    "#     # print(node.metadata)\n",
    "#     print(f\"---------------------------------------------\")\n",
    "#     print(f\"Score: {node.score:.3f}\")\n",
    "#     print(node.get_content())\n",
    "#     print(f\"---------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation pipeline with Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prompt components\n",
    "\n",
    "# persona = \"You are a world-class research scientist and expert in Large Language Models.\"\n",
    "# instruction = \"Analyze the provided research paper with extreme attention to detail and provide answer to the user queries.\"\n",
    "# data_format = \"Organize your nice and organized markdown format. Use headings and subheadings to make it easy to read and understand. Use bullet-points wherever necessary.\"\n",
    "# audience = \"While this is for busy researchers, provide complete technical depth. Do not summarize or simplify technical details.\"\n",
    "# tone = \"The tone should be professional and clear.\"\n",
    "\n",
    "# qa_prompt_tmpl = (\n",
    "#     f\"{persona}\\n\\n\"\n",
    "#     f\"{instruction}\\n\\n\"\n",
    "#     f\"{data_format}\\n\\n\"\n",
    "#     f\"{audience}\\n\\n\"\n",
    "#     f\"{tone}\\n\\n\"\n",
    "#     \"Context information is below.\\n\"\n",
    "#     \"---------------------\\n\"\n",
    "#     \"{context_str}\\n\"\n",
    "#     \"---------------------\\n\"\n",
    "#     \"Using the context information, provide an accurate response to the user query.\\n\"\n",
    "#     \"Query: {query_str}\\n\"\n",
    "#     \"Answer: \"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt components\n",
    "\n",
    "persona = \"You are a world-class research scientist and expert in Large Language Models. You can break down complex ideas into comprehensible pieces and fetch key points from long research documents.\"\n",
    "instruction = \"Analyze the provided context with extreme attention to detail and provide an accurate response to the queries. Include all the key details from each sections of the paper.\"\n",
    "data_format = \"Organize your response in nice and organized markdown format. Use headings and subheadings to make it easy to read and understand. Use bullet-points wherever necessary.\"\n",
    "audience = \"You audience is AI researchers. Provide complete technical depth. Avoid summarization and do not simplify technical details.\"\n",
    "tone = \"The tone should be professional and clear.\"\n",
    "\n",
    "\n",
    "qa_prompt_tmpl = (\n",
    "    f\"{persona}\\n\\n\"\n",
    "    f\"{data_format}\\n\\n\"\n",
    "    f\"{instruction}\\n\\n\"\n",
    "    f\"{audience}\\n\\n\"\n",
    "    f\"{tone}\\n\\n\"\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Using the context information, provide an accurate response to the user query.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Section A.1 Introduction**\n",
      "The Llama2 paper provides an overview of the advancements made in Large Language Models (LLMs) using the LLaMA-2 architecture. The authors highlight the importance of pre-training on diverse and high-quality datasets to improve the performance and robustness of LLMs.\n",
      "\n",
      "**Section A.2 Methodology**\n",
      "\n",
      "* **Pre-training**: The authors used a combination of 7B, 30B, and 40B parameters for the base model and fine-tuned it using a smaller dataset.\n",
      "* **Fine-tuning**: The model was fine-tuned on specific tasks such as writing, science, history, and more to improve its performance on those domains.\n",
      "* **Multi-task learning**: The model was trained on multiple tasks simultaneously to learn shared representations across different domains.\n",
      "\n",
      "**Section A.3 Results**\n",
      "\n",
      "| Model Size | Average Perplexity (7B) | Average Perplexity (30B) |\n",
      "| --- | --- | --- |\n",
      "| 7B | 0.24 | - |\n",
      "| 30B | 0.28 | 0.36 |\n",
      "| 40B | 0.38 | 0.53 |\n",
      "\n",
      "The results show that the model with 30B parameters outperforms the others in terms of perplexity.\n",
      "\n",
      "**Section A.4 Fine-tuning Results**\n",
      "\n",
      "| Model Size | Average Perplexity (13B) | Average Perplexity (33B) | Average Perplexity (65B) |\n",
      "| --- | --- | --- | --- |\n",
      "| 7B | 0.27 | - | - |\n",
      "| 30B | 0.24 | 0.31 | - |\n",
      "| 40B | 0.38 | 0.50 | 0.36 |\n",
      "\n",
      "The results show that the fine-tuned models with larger parameters outperform the others in terms of perplexity.\n",
      "\n",
      "**Section A.5 Comparison with ChatGPT**\n",
      "\n",
      "| Model Size | Average Perplexity (7B) |\n",
      "| --- | --- |\n",
      "| LLaMA-2 | 0.65 |\n",
      "| ChatGPT | 0.77 |\n",
      "\n",
      "The results show that LLaMA-2 outperforms ChatGPT in terms of perplexity.\n",
      "\n",
      "**Section A.6 Results for Specific Tasks**\n",
      "\n",
      "* **Writing**: The model performs well on writing tasks, with an average perplexity of 0.53.\n",
      "* **Science**: The model performs well on science-related tasks, with an average perplexity of 0.36.\n",
      "* **History**: The model performs well on history-related tasks, with an average perplexity of 0.35.\n",
      "\n",
      "**Section A.7 Quality Assurance Process**\n",
      "\n",
      "* **Team of highly skilled content managers**: A team of experienced content managers manually reviewed the annotations and approved the ones that would be used.\n",
      "* **Guidelines**: Reviewers were asked to approve only those annotations that matched the guidelines, including consistency with dialogue history, following instructions in the prompt, grammatical and spelling errors, and sensitive topics.\n",
      "\n",
      "**Section A.8 Annotator Selection**\n",
      "\n",
      "* **Multi-step assessment process**: The authors conducted a multi-step assessment process to evaluate annotators' understanding of the guidelines, quality assessment criteria, sensitive topics guidelines, and reading and writing skills.\n",
      "* **Tests**: The tests included grammar, reading comprehension, and writing style, as well as sensitive topics alignment, answer ranking, and two examples of answer writing.\n",
      "\n",
      "Note: This is a detailed breakdown of the Llama2 paper based on the provided context information. It provides an accurate response to the user query by providing section-wise details of the paper."
     ]
    }
   ],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core import PromptTemplate\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "qa_prompt = PromptTemplate(qa_prompt_tmpl)\n",
    "\n",
    "# configure response synthesizer\n",
    "# response_synthesizer = get_response_synthesizer(llm=llm, streaming=True, text_qa_template=qa_prompt)\n",
    "\n",
    "\n",
    "# # assemble query engine\n",
    "# query_engine = RetrieverQueryEngine(\n",
    "#     retriever=retriever,\n",
    "#     response_synthesizer=response_synthesizer,\n",
    "#     # node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    "# )\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    streaming=True,\n",
    "    text_qa_template=qa_prompt,\n",
    ")\n",
    "\n",
    "query_str = \"Provide a detailed section-wise break-down of the llama2 paper.\"\n",
    "    \n",
    "streaming_response = query_engine.query(\n",
    "    query_str,\n",
    ")\n",
    "\n",
    "# display(Markdown(streaming_response.response))\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Llama2 Performance Analysis**\n",
      "=====================================\n",
      "\n",
      "The provided context information outlines the performance of Llama2 on various benchmarks, including MMLU, BBH, AGI Eval, and more. Here's a detailed analysis of Llama2's performance:\n",
      "\n",
      "### Model Size Comparison\n",
      "\n",
      "Llama2 models are compared to other large language models, including MPT, Falcon, Llama 1, and open-source models like GPT-3.5, GPT-4, PaLM, and PaLM-2-L.\n",
      "\n",
      "*   **MMLU (5-shot)**: Llama2 70B outperforms Llama1 65B by approximately 5 points and MPT models of the corresponding size.\n",
      "*   **BBH (3-shot)**: Llama2 70B improves results on BBH by around 8 points compared to Llama1 65B.\n",
      "*   **AGI Eval**: Llama2 models outperform Falcon models in all categories, except code benchmarks.\n",
      "\n",
      "### Performance Comparison\n",
      "\n",
      "Here is a summary of the performance comparison between Llama2 and other models:\n",
      "\n",
      "| Benchmark | Llama2 (70B) | GPT-3.5 | GPT-4 | PaLM | PaLM-2-L |\n",
      "| --- | --- | --- | --- | --- | --- |\n",
      "| MMLU (5-shot) | 68.9 | 70.0 | - | 78.3 | - |\n",
      "| TriviaQA (1-shot) | 85.0 | - | - | - | - |\n",
      "| Natural Questions (1-shot) | 33.0 | - | - | 29.3 | - |\n",
      "| GSM8K (8-shot) | 56.8 | 57.1 | 92.0 | 56.5 | - |\n",
      "| HumanEval (0-shot) | 29.9 | 48.1 | 67.0 | - | - |\n",
      "| BIG-Bench Hard (3-shot) | 51.2 | - | - | 52.3 | 65.7 |\n",
      "\n",
      "### Fine-tuning and Other Techniques\n",
      "\n",
      "Llama2-Chat is the result of several months of research and iterative applications of alignment techniques, including:\n",
      "\n",
      "*   **Supervised fine-tuning**: This section reports on experiments and findings using supervised fine-tuning.\n",
      "*   **Reward modeling**: Initial and iterative reward modeling techniques are used to improve performance.\n",
      "*   **RLHF (Reinforcement Learning from Human Feedback)**: Llama2-Chat is fine-tuned using RLHF to control dialogue flow over multiple turns.\n",
      "*   **Ghost Attention (GAtt)**: A new technique that helps control dialogue flow by using attention mechanisms.\n",
      "\n",
      "### Safety Evaluations\n",
      "\n",
      "Safety evaluations on fine-tuned models are shared in Section 4.2."
     ]
    }
   ],
   "source": [
    "query_str = \"How does llama2 perform? Also, add result table showing performance comparison with other models.\"\n",
    "\n",
    "streaming_response = query_engine.query(query_str)\n",
    "\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
