{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Codebase Security Analyzer with Gemini-1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook implements a sophisticated security analysis tool that leverages Google's Gemini 1.5 Pro LLM to perform comprehensive security assessments of Python codebases. The analyzer processes entire repositories, examining multiple file types including Python source code, configuration files, Docker configurations, CI/CD pipelines, dependencies, and potential secret-containing files. It utilizes Gemini's large context window to analyze security vulnerabilities, coding patterns, and potential risks across the entire codebase.\n",
    "\n",
    "**Key Features**\n",
    "\n",
    "- Holistic repository analysis with support for multiple file types\n",
    "- Advanced security vulnerability detection using Gemini 1.5 Pro\n",
    "- Categorized analysis of different file types (code, configs, Docker, CI/CD, etc.)\n",
    "- Intelligent handling of potential secret-containing files\n",
    "- Comprehensive security report generation in Markdown format\n",
    "- Built-in retry mechanisms and error handling\n",
    "- Support for large codebases with efficient file processing\n",
    "\n",
    "The tool is designed to help security analysts, developers, and DevOps teams identify potential security issues early in the development lifecycle, providing actionable insights and recommendations for improving codebase security."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Installation\n",
    "First, we will install the required packages and and set up our generation api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "\n",
    "# !pip install google-generativeai tenacity gitpython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import fnmatch\n",
    "import tempfile\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import git\n",
    "import google.generativeai as genai\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Security Analyzer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ComprehensiveSecurityAnalyzer:\n",
    "    \"\"\"A comprehensive security analysis tool that leverages Gemini 1.5 Pro to analyze entire codebases for security vulnerabilities. \n",
    "    \n",
    "    This class handles repository cloning, file categorization, and security analysis across multiple file types including source code, configurations, Docker files, CI/CD configurations, dependencies, documentation, and potential secret-containing files.\n",
    "    \n",
    "    The analyzer uses pattern matching and pathlib for efficient file processing, implements retry mechanisms for API reliability, and generates detailed security reports in Markdown format. It's designed to process files up to 1MB in size and uses UTF-8  encoding with error handling for maximum compatibility.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    gemini_api_key : str\n",
    "        API key for the Gemini 1.5 Pro model.\n",
    "    repo_path : Optional[str], default=None\n",
    "        Path to the cloned repository.\n",
    "    temp_dir : str, default=tempfile.mkdtemp(dir=\".\")\n",
    "        Temporary directory to store cloned repositories and analysis results.\n",
    "    model : genai.GenerativeModel\n",
    "        Instance of the Gemini 1.5 model.\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    clone_repository(repo_url: str) -> Path\n",
    "        Clone a repository from a given URL.\n",
    "    gather_relevant_files() -> dict[str, list[dict[str, str]]]\n",
    "        Gather relevant files from the cloned repository.\n",
    "    analyze_security() -> str\n",
    "        Perform security analysis category by category.\n",
    "    cleanup()\n",
    "        Clean up temporary files.\n",
    "    \"\"\"\n",
    "\n",
    "    gemini_api_key: str\n",
    "    repo_path: Optional[str] = None\n",
    "    temp_dir: str = field(default_factory=lambda: tempfile.mkdtemp(dir=\".\"))\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # configure gemini model\n",
    "        genai.configure(api_key=self.gemini_api_key)\n",
    "        self.model = genai.GenerativeModel(\n",
    "            model_name=\"models/gemini-1.5-pro-001\",\n",
    "        )\n",
    "\n",
    "    # Define file patterns to analyze\n",
    "    # We will use these in our prompt later\n",
    "    file_patterns = {\n",
    "        \"code_files\": {\n",
    "            \".py\",  # Python source\n",
    "            \".pyx\",  # Cython source\n",
    "            \".pyi\",  # Python interface\n",
    "            \".ipynb\",  # Jupyter notebook\n",
    "        },\n",
    "        \"config_files\": {\n",
    "            \".yml\",\n",
    "            \".yaml\",\n",
    "            \".json\",\n",
    "            \".env\",\n",
    "            \".ini\",\n",
    "            \".toml\",\n",
    "            \".cfg\",\n",
    "            \".conf\",\n",
    "            \".properties\",\n",
    "        },\n",
    "        \"docker_files\": {\"Dockerfile\", \"docker-compose.yml\", \"docker-compose.yaml\"},\n",
    "        \"ci_files\": {\n",
    "            \".github/workflows/*.yml\",\n",
    "            \".github/workflows/*.yaml\",\n",
    "            \"**/Jenkinsfile\",\n",
    "            \".gitlab-ci.yml\",\n",
    "            \"azure-pipelines.yml\",\n",
    "            \".circleci/config.yml\",\n",
    "            \".travis.yml\",\n",
    "        },\n",
    "        \"dependency_files\": {\n",
    "            \"requirements.txt\",\n",
    "            \"setup.py\",\n",
    "            \"pyproject.toml\",\n",
    "            \"Pipfile\",\n",
    "            \"poetry.lock\",\n",
    "            \"requirements/*.txt\",\n",
    "            \"requirements/**/*.txt\",\n",
    "            \"setup.cfg\",\n",
    "            \"constraints.txt\",\n",
    "        },\n",
    "        \"docs\": {\".md\", \".rst\", \".txt\", \".pdf\", \".doc\", \".docx\"},\n",
    "        \"secrets\": {\n",
    "            \"**/*.env\",\n",
    "            \".env.*\",\n",
    "            \"**/*.pem\",\n",
    "            \"**/*.key\",\n",
    "            \"**/secrets.*\",\n",
    "            \"**/*.cert\",\n",
    "            \"**/*.p12\",\n",
    "            \"**/*.pfx\",\n",
    "            \"**/id_rsa\",\n",
    "            \"**/id_dsa\",\n",
    "            \"**/id_ecdsa\",\n",
    "            \"**/id_ed25519\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    def clone_repository(self, repo_url: str) -> Path:\n",
    "        \"\"\"Clone a repository from a given URL.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        repo_url : str\n",
    "            URL of the repository to clone.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Path\n",
    "            Path to the cloned repository.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        Exception\n",
    "            If the repository cloning fails.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            if not self.repo_path:\n",
    "                self.repo_path = Path(self.temp_dir) / Path(repo_url).name\n",
    "                git.Repo.clone_from(repo_url, self.repo_path)\n",
    "                return self.repo_path\n",
    "            else:\n",
    "                print(\"Repository already cloned.\")\n",
    "        except git.GitCommandError as e:\n",
    "            raise Exception(f\"Failed to clone repository: {str(e)}\")\n",
    "\n",
    "    def gather_relevant_files(self) -> dict[str, list[dict[str, str]]]:\n",
    "        \"\"\"Gather relevant files from the cloned repository.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict[str, list[dict[str, str]]]\n",
    "            A dictionary containing files grouped by category.\n",
    "        \"\"\"\n",
    "        files_by_category = {\n",
    "            \"code\": [],\n",
    "            \"config\": [],\n",
    "            \"docker\": [],\n",
    "            \"ci_cd\": [],\n",
    "            \"dependencies\": [],\n",
    "            \"documentation\": [],\n",
    "            \"potential_secrets\": [],\n",
    "        }\n",
    "\n",
    "        # Skip common non-source directories\n",
    "        skip_dirs = {\".git\", \"build\", \"dist\", \"__pycache__\", \"*.egg-info\"}\n",
    "\n",
    "        # Use pathlib's rglob to recursively get all files\n",
    "        for file_path in Path(self.repo_path).rglob(\"*\"):\n",
    "            # Skip directories and files in skip_dirs\n",
    "            if any(parent.name in skip_dirs for parent in file_path.parents):\n",
    "                continue\n",
    "\n",
    "            # Skip directories, only process files\n",
    "            if not file_path.is_file():\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Skip large files (> 1MB)\n",
    "                if file_path.stat().st_size > 1_000_000:\n",
    "                    continue\n",
    "\n",
    "                # Get relative path\n",
    "                relative_path = file_path.relative_to(self.repo_path)\n",
    "\n",
    "                # Read file content\n",
    "                try:\n",
    "                    content = file_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "                except UnicodeDecodeError:\n",
    "                    # Skip binary files\n",
    "                    continue\n",
    "\n",
    "                file_info = {\"path\": str(relative_path), \"content\": content}\n",
    "\n",
    "                # Categorize file using pattern matching\n",
    "                category = self._categorize_file(str(relative_path))\n",
    "                if category:\n",
    "                    files_by_category[category].append(file_info)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "        return files_by_category\n",
    "\n",
    "    def _categorize_file(self, file_path: str) -> str:\n",
    "        \"\"\"Categorize a file based on its path.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_path : str\n",
    "            Path of the file to categorize.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Category of the file.\n",
    "        \"\"\"\n",
    "        path = Path(file_path)\n",
    "\n",
    "        match path:\n",
    "            # Code files\n",
    "            case _ if path.suffix in self.file_patterns[\"code_files\"]:\n",
    "                return \"code\"\n",
    "\n",
    "            # Config files\n",
    "            case _ if path.suffix in self.file_patterns[\"config_files\"]:\n",
    "                return \"config\"\n",
    "\n",
    "            # Docker files\n",
    "            case _ if path.name in self.file_patterns[\"docker_files\"]:\n",
    "                return \"docker\"\n",
    "\n",
    "            # CI/CD files\n",
    "            case _ if any(\n",
    "                fnmatch.fnmatch(file_path, pattern) for pattern in self.file_patterns[\"ci_files\"]\n",
    "            ):\n",
    "                return \"ci_cd\"\n",
    "\n",
    "            # Dependency files\n",
    "            case _ if path.name in self.file_patterns[\"dependency_files\"]:\n",
    "                return \"dependencies\"\n",
    "\n",
    "            # Documentation files\n",
    "            case _ if path.suffix in self.file_patterns[\"docs\"]:\n",
    "                return \"documentation\"\n",
    "\n",
    "            # Potential secrets\n",
    "            case _ if any(\n",
    "                fnmatch.fnmatch(file_path, pattern) for pattern in self.file_patterns[\"secrets\"]\n",
    "            ):\n",
    "                return \"potential_secrets\"\n",
    "\n",
    "            # No match\n",
    "            case _:\n",
    "                return None\n",
    "\n",
    "    @retry(\n",
    "        stop=stop_after_attempt(3),\n",
    "        wait=wait_exponential(multiplier=1, min=4, max=10),\n",
    "        reraise=True,\n",
    "    )\n",
    "    def _analyze_category(self, category: str, files: list, retry_count: int | None = 3) -> str:\n",
    "        \"\"\"Analyze a category of files for security vulnerabilities.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        category : str\n",
    "            Category of files to analyze.\n",
    "        files : list\n",
    "            List of files to analyze.\n",
    "        retry_count : int, optional\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Analysis report in Markdown format.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Generate prompt for the category\n",
    "            category_files = \"\\n\\n\".join(\n",
    "                f\"### {file['path']}\\n```\\n{file['content']}\\n```\" for file in files\n",
    "            )\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "            Analyze these {category.upper()} FILES for security vulnerabilities:\n",
    "\n",
    "            Files to analyze:\n",
    "            {category_files}\n",
    "\n",
    "            Provide analysis focusing on relevant security aspects for {category} files:\n",
    "\n",
    "            {self._get_category_checklist(category)}\n",
    "\n",
    "            Format the response in Markdown with:\n",
    "            1. CRITICAL FINDINGS\n",
    "            2. HIGH-RISK ISSUES\n",
    "            3. MEDIUM CONCERNS\n",
    "            4. RECOMMENDATIONS\n",
    "            \"\"\"\n",
    "\n",
    "            # Generate content using the model\n",
    "            response = self.model.generate_content(prompt)\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            if retry_count > 0:\n",
    "                print(f\"Retrying {category} analysis... ({retry_count} attempts left)\")\n",
    "                time.sleep(5)  # Add delay between retries\n",
    "                return self._analyze_category(category, files, retry_count - 1)\n",
    "            raise e\n",
    "\n",
    "    def _get_category_checklist(self, category: str) -> str:\n",
    "        \"\"\"Get a checklist of security aspects for a given category.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        category : str\n",
    "            Category of files to analyze.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Checklist of security aspects for the category.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        checklists = {\n",
    "            \"code\": \"\"\"\n",
    "                - Input validation\n",
    "                - Authentication/Authorization\n",
    "                - Data sanitization\n",
    "                - Secure coding practices\n",
    "                - Error handling\n",
    "                - Cryptographic implementations\n",
    "            \"\"\",\n",
    "            \"config\": \"\"\"\n",
    "                - Hardcoded credentials\n",
    "                - Insecure defaults\n",
    "                - Exposed sensitive information\n",
    "                - Misconfiguration risks\n",
    "            \"\"\",\n",
    "            \"docker\": \"\"\"\n",
    "                - Root/privileged execution\n",
    "                - Exposed ports\n",
    "                - Base image security\n",
    "                - Build-time secrets\n",
    "            \"\"\",\n",
    "            \"ci_cd\": \"\"\"\n",
    "                - Secret management\n",
    "                - Secure pipeline practices\n",
    "                - Security testing integration\n",
    "            \"\"\",\n",
    "            \"dependencies\": \"\"\"\n",
    "                - Known vulnerable dependencies\n",
    "                - Supply chain risks\n",
    "                - Version constraints\n",
    "            \"\"\",\n",
    "            \"documentation\": \"\"\"\n",
    "                - Exposed sensitive info\n",
    "                - Security misconfigurations\n",
    "                - Outdated practices\n",
    "            \"\"\",\n",
    "            \"potential_secrets\": \"\"\"\n",
    "                - API keys\n",
    "                - Access tokens\n",
    "                - Credentials\n",
    "                - Private keys\n",
    "            \"\"\",\n",
    "        }\n",
    "        return checklists.get(category, \"\")\n",
    "\n",
    "    def analyze_security(self) -> str:\n",
    "        \"\"\"Perform security analysis category by category\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to the generated security analysis report.\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        - The report is generated in Markdown format.\n",
    "        - We are generating report separately for each category. I faced quota and timeout issues when generating report for all categories at once.\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            files_by_category = self.gather_relevant_files()\n",
    "            report_path = Path(f\"./security_analysis_{Path(self.repo_path).name}.md\")\n",
    "\n",
    "            # Create report header\n",
    "            report_sections = [\n",
    "                \"# Comprehensive Security Analysis Report\\n\",\n",
    "                f\"## Repository: {Path(self.repo_path).name}\\n\",\n",
    "                f\"### Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\",\n",
    "            ]\n",
    "\n",
    "            # Analyze each category separately\n",
    "            category_analyses = []\n",
    "            for category, files in files_by_category.items():\n",
    "                if not files:\n",
    "                    continue\n",
    "\n",
    "                print(f\"\\nAnalyzing {category} files...\")\n",
    "                try:\n",
    "                    analysis = self._analyze_category(category, files)\n",
    "                    category_analyses.append(\n",
    "                        f\"\\n## {category.title()} Security Analysis\\n{analysis}\"\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error analyzing {category}: {str(e)}\")\n",
    "                    category_analyses.append(\n",
    "                        f\"\\n## {category.title()} Security Analysis\\nAnalysis failed: {str(e)}\"\n",
    "                    )\n",
    "\n",
    "                # Add delay between categories\n",
    "                time.sleep(2)\n",
    "\n",
    "            # Add category analyses to report\n",
    "            report_sections.extend(category_analyses)\n",
    "\n",
    "            # Generate cross-cutting summary\n",
    "            try:\n",
    "                summary_prompt = \"\"\"\n",
    "                    Based on the above category-specific analyses, provide a high-level security assessment focusing on:\n",
    "\n",
    "                    1. EXECUTIVE SUMMARY\n",
    "                    2. CROSS-CUTTING CONCERNS\n",
    "                    3. CRITICAL PATTERNS\n",
    "                    4. KEY RECOMMENDATIONS\n",
    "\n",
    "                    Use proper Markdown formatting.\n",
    "                 \"\"\"\n",
    "                 \n",
    "                summary = self.model.generate_content(\n",
    "                    summary_prompt + \"\\n\\n\" + \"\\n\".join(category_analyses[-1000:])\n",
    "                )\n",
    "                report_sections.extend([\"\\n## Cross-Cutting Analysis\\n\", summary.text])\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating summary: {str(e)}\")\n",
    "\n",
    "            # Write report\n",
    "            report_content = \"\\n\".join(report_sections)\n",
    "            report_path.write_text(report_content, encoding=\"utf-8\")\n",
    "            print(f\"\\nSecurity analysis report generated: {report_path}\")\n",
    "            return str(report_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error during security analysis: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            report_sections.extend([\"\\n## Error During Analysis\\n\", f\"```\\n{error_msg}\\n```\"])\n",
    "            report_path.write_text(\"\\n\".join(report_sections), encoding=\"utf-8\")\n",
    "            return str(report_path)\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up temporary files.\"\"\"\n",
    "        if self.repo_path and Path(self.repo_path).exists():\n",
    "            import shutil\n",
    "\n",
    "            shutil.rmtree(self.temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Security Analysis\n",
    "\n",
    "In this section, we'll conduct a thorough security analysis of the FastAPI repository using our analyzer. FastAPI was chosen as our test case for several compelling reasons:\n",
    "\n",
    "1. **Modern Python Framework:** FastAPI is a modern, fast web framework for building APIs with Python, making it an excellent real-world test case\n",
    "2. **Large Active Codebase:** With over 60k stars on GitHub, it represents a production-grade codebase with real security considerations\n",
    "3. **Complex Dependencies:** Features integration with various components like Pydantic, Starlette, and authentication systems\n",
    "4. **Security-Critical:** As a web framework, FastAPI's security practices directly impact thousands of production applications\n",
    "5. **Well-Documented:** Its comprehensive documentation allows us to validate our analysis against known security patterns\n",
    "\n",
    "Our analysis will focus on identifying potential security vulnerabilities across different components of the framework, from core routing logic to authentication mechanisms and dependency management. We'll use Gemini 1.5 Pro's capabilities to analyze both the code and its surrounding infrastructure, providing insights that could benefit the broader FastAPI community.\n",
    "Let's proceed with the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing code files...\n",
      "\n",
      "Analyzing config files...\n",
      "\n",
      "Analyzing docker files...\n",
      "\n",
      "Analyzing dependencies files...\n",
      "\n",
      "Analyzing documentation files...\n",
      "\n",
      "Security analysis report generated: security_analysis_fastapi.git.md\n"
     ]
    }
   ],
   "source": [
    "# Initialize the security analyzer\n",
    "api_key = UserSecretsClient().get_secret(\"GEMINI_API_KEY\")\n",
    "analyzer = ComprehensiveSecurityAnalyzer(\n",
    "    gemini_api_key=api_key,\n",
    ")\n",
    "\n",
    "# Clone a repository and analyze security\n",
    "analyzer.clone_repository(repo_url=\"https://github.com/fastapi/fastapi.git\")\n",
    "analyzer.analyze_security()\n",
    "\n",
    "# Clean up temporary files\n",
    "analyzer.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the generated report. The report should provide a comprehensive security analysis of the repository, including detailed findings and recommendations for each category of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Comprehensive Security Analysis Report\n",
       "\n",
       "## Repository: fastapi.git\n",
       "\n",
       "### Analysis Date: 2024-11-24 21:49:19\n",
       "\n",
       "## Code Security Analysis\n",
       "\n",
       "## FastAPI Code Analysis: Security Review\n",
       "\n",
       "This markdown document provides a security analysis of the provided FastAPI code files.\n",
       "\n",
       "**1. CRITICAL FINDINGS**\n",
       "\n",
       "- **No critical findings detected.** The code does not appear to handle sensitive data in a way that would expose it to critical vulnerabilities like SQL injection or remote code execution.\n",
       "\n",
       "**2. HIGH-RISK ISSUES**\n",
       "\n",
       "- **Potential for Unrestricted File Uploads (tests/test_custom_middleware_exception.py):** The custom middleware in `test_custom_middleware_exception.py` only checks the content length. Without additional checks on file type and content, malicious users could upload executable files, potentially leading to remote code execution.\n",
       "\n",
       "**3. MEDIUM CONCERNS**\n",
       "\n",
       "- **Missing Authentication/Authorization in WebSockets (tests/test_ws_router.py):** While some WebSocket routes utilize dependencies for potential authentication, other routes like `/`, `/router`, `/prefix/`, `/native/`, and `/router2` accept connections without any authentication or authorization checks. This could lead to unauthorized access and data manipulation.\n",
       "- **Unvalidated Redirect (tests/test_custom_swagger_ui_redirect.py):** The `/docs/redirect` endpoint used for OAuth2 redirect in Swagger UI does not validate the redirect URL. A malicious actor could potentially manipulate this redirect to send users to a malicious website.\n",
       "\n",
       "**4. RECOMMENDATIONS**\n",
       "\n",
       "- **Implement Strict Input Validation:** Thoroughly validate all user inputs, including request bodies, query parameters, headers, and uploaded files. Leverage Pydantic's capabilities for type checking, constraints, and custom validators to enforce data integrity and prevent malicious inputs.\n",
       "- **Enforce Authentication and Authorization:** Implement robust authentication and authorization mechanisms for all endpoints, including WebSockets. Consider using industry-standard protocols like OAuth2 or JWT for token-based authentication. Implement role-based access control (RBAC) to manage user permissions.\n",
       "- **Secure File Uploads:** In addition to size checks, implement file type validation and content sanitization for uploaded files. Store uploaded files in a secure location outside the web root.\n",
       "- **Validate Redirects:** Validate redirect URLs to ensure they point to trusted domains. Avoid using user-supplied data directly in redirect URLs.\n",
       "- **Harden Error Handling:** Avoid revealing sensitive information in error messages. Implement generic error messages for unexpected exceptions to prevent information leakage.\n",
       "- **Consider Security Best Practices:** Follow secure coding practices such as parameterizing queries, avoiding hardcoded credentials, and keeping dependencies updated to mitigate potential vulnerabilities.\n",
       "- **Use Security Linters:** Employ security-focused linters like Bandit or Snyk to identify potential security issues during development.\n",
       "\n",
       "By addressing these recommendations, you can significantly enhance the security posture of your FastAPI application.\n",
       "\n",
       "## Config Security Analysis\n",
       "\n",
       "## Analysis of Configuration Files for Security Vulnerabilities\n",
       "\n",
       "Here's a security analysis of the provided configuration files:\n",
       "\n",
       "### 1. CRITICAL FINDINGS\n",
       "\n",
       "- **None:** No critical security vulnerabilities were found in the provided configuration files.\n",
       "\n",
       "### 2. HIGH-RISK ISSUES\n",
       "\n",
       "- **None:** No high-risk security issues were found in the provided configuration files.\n",
       "\n",
       "### 3. MEDIUM CONCERNS\n",
       "\n",
       "- **`.pre-commit-config.yaml` - `check-yaml` with `--unsafe` argument:** While this configuration enhances flexibility, it disables YAML schema validation, potentially allowing malicious YAML payloads to execute arbitrary code. This risk depends on how the YAML files are processed within the project.\n",
       "\n",
       "### 4. RECOMMENDATIONS\n",
       "\n",
       "- **Review the use of `--unsafe` in `.pre-commit-config.yaml`:** Consider if the flexibility provided by disabling YAML schema validation outweighs the potential security risks. If possible, define and use a safe YAML schema for your project and remove the `--unsafe` flag.\n",
       "- **Regularly update dependencies:** Several configuration files specify dependency versions. Regularly update these dependencies to benefit from security patches and bug fixes. Use tools like `dependabot` (already configured in `dependabot.yml`) to automate this process.\n",
       "- **Secure secrets:** While no hardcoded credentials were identified in these files, ensure that any sensitive information like API keys or tokens are stored securely, preferably using environment variables or a dedicated secret management system.\n",
       "- **Principle of least privilege:** The GitHub Actions workflows have granular permission settings. Maintain this practice to minimize the potential impact of compromised workflows.\n",
       "\n",
       "**Overall:** The configuration files demonstrate good security practices with no critical or high-risk vulnerabilities detected. The medium concern highlighted should be evaluated in the context of the project's specific use of YAML files.\n",
       "\n",
       "## Docker Security Analysis\n",
       "\n",
       "## Dockerfile Security Analysis\n",
       "\n",
       "Here's a breakdown of potential security concerns in the provided Dockerfiles:\n",
       "\n",
       "**1. CRITICAL FINDINGS**\n",
       "\n",
       "- **None.** The provided Dockerfiles don't contain immediately exploitable critical vulnerabilities.\n",
       "\n",
       "**2. HIGH-RISK ISSUES**\n",
       "\n",
       "- **Root/Privileged Execution:** Both Dockerfiles use `FROM python:3.9` without specifying a non-root user. This means the application runs as root inside the container, which poses a significant security risk. If the application is compromised, the attacker gains root access to the container and potentially the host system.\n",
       "\n",
       "**3. MEDIUM CONCERNS**\n",
       "\n",
       "- **Base Image Security:** While using official images like `python:3.9` is generally good practice, it's essential to use specific tags (e.g., `python:3.9.13-slim-bullseye`) instead of just the major version. This ensures you're using a particular version with known vulnerabilities patched. Regularly update the base image to benefit from the latest security fixes.\n",
       "\n",
       "- **Dependency Pinning:** Both Dockerfiles use version ranges for some packages (e.g., `\"pyyaml>=5.3.1,<6.0.0\"`). While this provides flexibility, it can lead to unpredictable behavior and potential vulnerabilities if new versions introduce breaking changes or security flaws.\n",
       "\n",
       "**4. RECOMMENDATIONS**\n",
       "\n",
       "- **Run as Non-Root User:**\n",
       "  - Create a dedicated user and group in the Dockerfile:\n",
       "    ```dockerfile\n",
       "    RUN addgroup --system appuser && adduser --system --ingroup appuser --no-create-home appuser\n",
       "    ```\n",
       "  - Set the user for running the application:\n",
       "    ```dockerfile\n",
       "    USER appuser\n",
       "    ```\n",
       "- **Use Specific Base Image Tags:**\n",
       "  - Instead of `FROM python:3.9`, use a specific tag:\n",
       "    ```dockerfile\n",
       "    FROM python:3.9.13-slim-bullseye\n",
       "    ```\n",
       "- **Pin Dependency Versions:**\n",
       "  - Specify exact versions for all dependencies to ensure consistent builds and reduce the risk of vulnerabilities from unanticipated updates. For example:\n",
       "    ```dockerfile\n",
       "    RUN pip install httpx PyGithub \"pydantic==2.0.2\" pydantic-settings \"pyyaml==5.4.1\"\n",
       "    ```\n",
       "- **Least Privilege Principle:**\n",
       "  - Review the application's permissions and ensure it only has access to the resources it absolutely needs.\n",
       "- **Image Scanning:**\n",
       "  - Integrate a vulnerability scanner (e.g., Trivy, Snyk, Clair) into your CI/CD pipeline to automatically scan images for known vulnerabilities.\n",
       "- **Regular Updates:**\n",
       "  - Establish a process for regularly updating base images and dependencies to patch vulnerabilities.\n",
       "\n",
       "By addressing these concerns, you can significantly improve the security posture of your Docker images and reduce the attack surface.\n",
       "\n",
       "## Dependencies Security Analysis\n",
       "\n",
       "## Security Analysis of `requirements.txt`\n",
       "\n",
       "This analysis focuses on the provided `requirements.txt` snippet and assesses potential security risks.\n",
       "\n",
       "**1. CRITICAL FINDINGS**\n",
       "\n",
       "- **None:** There are no critical findings based on the provided information.\n",
       "\n",
       "**2. HIGH-RISK ISSUES**\n",
       "\n",
       "- **Open-ended dependency:** The line `playwright` without a version constraint introduces a high-risk issue. Installing without specifying a version range allows for the possibility of automatically installing a new, potentially vulnerable version of Playwright in the future.\n",
       "\n",
       "**3. MEDIUM CONCERNS**\n",
       "\n",
       "- **Local dependency:** The line `-e .[all]` indicates installation from a local directory. While this is not inherently insecure, it presents a supply chain risk if the local codebase is compromised, potentially leading to the installation of malicious code.\n",
       "\n",
       "- **Indirect dependencies:** The files `requirements-tests.txt` and `requirements-docs.txt` might introduce indirect vulnerabilities depending on the packages listed and their versions. Without analyzing their content, a complete assessment is impossible.\n",
       "\n",
       "**4. RECOMMENDATIONS**\n",
       "\n",
       "- **Pin Playwright version:** **Immediately** specify a version or version range for the `playwright` dependency to prevent accidental installation of vulnerable versions in the future. For example:\n",
       "\n",
       "  ```\n",
       "  playwright>=1.20.0,<1.22.0\n",
       "  ```\n",
       "\n",
       "  Choose a version range that aligns with your project's needs while still receiving security updates.\n",
       "\n",
       "- **Secure local development environment:** If possible, avoid direct installation from the local directory (`-e .[all]`) in production environments. Consider building a package and installing from a trusted repository. Regularly audit your local codebase for vulnerabilities.\n",
       "\n",
       "- **Analyze included requirements files:** Thoroughly analyze the dependencies listed within `requirements-tests.txt` and `requirements-docs.txt`. Pin versions for all packages to minimize supply chain risks.\n",
       "\n",
       "- **Use a vulnerability scanner:** Integrate a vulnerability scanner like Snyk or OWASP Dependency-Check into your CI/CD pipeline to automatically detect known vulnerabilities in your dependencies and receive alerts for new threats.\n",
       "\n",
       "By implementing these recommendations, you can significantly improve the security posture of your project and mitigate potential risks associated with your dependencies.\n",
       "\n",
       "## Documentation Security Analysis\n",
       "\n",
       "## FastAPI Documentation Analysis: Security Vulnerabilities\n",
       "\n",
       "Here's a breakdown of potential security vulnerabilities based on the provided FastAPI documentation files:\n",
       "\n",
       "**1. CRITICAL FINDINGS**\n",
       "\n",
       "- **Exposure of Sensitive Information in `requirements-docs-insiders.txt`:** This file contains placeholders `${TOKEN}` for GitHub Personal Access Tokens (PATs). If this file is accidentally committed with actual PATs, it would lead to a critical vulnerability, allowing anyone with access to the repository to fully control the associated GitHub account.\n",
       "\n",
       "**2. HIGH-RISK ISSUES**\n",
       "\n",
       "- **None:** Based on the provided files, there are no immediate high-risk security issues other than the critical finding above.\n",
       "\n",
       "**3. MEDIUM CONCERNS**\n",
       "\n",
       "- **Potential for Unintended Exposure of Internal APIs:** The use of `app.internal` and comments indicating it's not \"mounted\" suggest the presence of internal APIs. Ensure these internal APIs are not unintentionally exposed to external users, especially in production environments. Consider robust access control measures specifically for internal APIs.\n",
       "\n",
       "- **Missing Specific Security Headers:** While Starlette, on which FastAPI is built, provides security features like `SessionMiddleware`, the provided code snippets do not explicitly demonstrate the use of specific security headers like `Content-Security-Policy`, `X-Frame-Options`, or `Strict-Transport-Security`. These headers enhance security against common web vulnerabilities.\n",
       "\n",
       "- **CORS Configuration Requires Careful Review:** The documentation mentions using the `\"*\"` wildcard for CORS `allow_origins` to allow all origins. While convenient, this opens up potential vulnerabilities, especially when dealing with sensitive user data or actions. Emphasize the importance of carefully evaluating and defining specific allowed origins for production environments.\n",
       "\n",
       "**4. RECOMMENDATIONS**\n",
       "\n",
       "- **Immediately Remove `${TOKEN}` Placeholders:** Replace `${TOKEN}` with clear instructions on how to obtain and use GitHub Personal Access Tokens for `requirements-docs-insiders.txt`. Never commit actual PATs to the repository. Consider using secrets management tools within your CI/CD pipeline if automation is required.\n",
       "\n",
       "- **Explicitly Document Best Practices for Secure Deployments:** Enhance the deployment documentation with clear guidance on utilizing TLS Termination Proxies, setting up HTTPS, and configuring servers for production environments. Emphasize the importance of security headers and provide specific examples of their usage with FastAPI.\n",
       "\n",
       "- **Provide Clearer Guidance on Handling Secrets:** Expand the documentation on environment variables and settings management, emphasizing best practices for handling sensitive information like API keys, database credentials, and other secrets. Consider recommending established secrets management tools or techniques.\n",
       "\n",
       "- **Offer Best Practice Examples for Internal API Protection:** Supplement the `APIRouter` and sub-application documentation with guidance on protecting internal APIs from unauthorized access. This could include authentication mechanisms, role-based access control, or network segmentation.\n",
       "\n",
       "- **Continuously Evaluate and Update Dependencies:** Regularly review and update dependencies, especially those with known vulnerabilities. Utilize tools like `pip-audit` or `safety` to help with vulnerability scanning.\n",
       "\n",
       "By addressing these recommendations, you can significantly strengthen the security of FastAPI applications and empower developers to build more secure APIs.\n",
       "\n",
       "## Cross-Cutting Analysis\n",
       "\n",
       "## FastAPI Application Security Assessment\n",
       "\n",
       "### 1. EXECUTIVE SUMMARY\n",
       "\n",
       "This assessment examined the security posture of a FastAPI application, encompassing code, configurations, Dockerfiles, dependencies, and documentation. While the application demonstrates a good understanding of basic security principles, several vulnerabilities and areas for improvement were identified.\n",
       "\n",
       "**Key Findings:**\n",
       "\n",
       "- **Potential for Unrestricted File Uploads:** Lack of robust file upload validation presents a significant risk for remote code execution.\n",
       "- **Insecure WebSocket Implementations:** Missing or inconsistent authentication and authorization in WebSockets create opportunities for unauthorized access.\n",
       "- **Unvalidated Redirects:** Susceptibility to open redirects could lead to phishing attacks.\n",
       "- **Exposure of Sensitive Information in Documentation:** Placeholders for GitHub Personal Access Tokens in documentation pose a critical risk if actual tokens are accidentally committed.\n",
       "\n",
       "**Overall Risk:** Medium\n",
       "\n",
       "### 2. CROSS-CUTTING CONCERNS\n",
       "\n",
       "- **Inconsistent Authentication & Authorization:** While some components employ authentication and authorization, this is not consistently applied across all endpoints and functionalities, especially WebSockets.\n",
       "- **Insufficient Input Validation:** A lack of comprehensive input validation increases the attack surface for various injection vulnerabilities (e.g., SQL injection, command injection).\n",
       "- **Lack of Security Hardening in Deployment Practices:** Documentation lacks guidance on secure deployment practices, including TLS configuration, security headers, and handling secrets in production.\n",
       "\n",
       "### 3. CRITICAL PATTERNS\n",
       "\n",
       "- **Reliance on Default Configurations:** Assuming secure defaults for components like CORS can lead to vulnerabilities. Explicitly configuring security settings is crucial.\n",
       "- **Missing Secure Coding Practices:** The codebase lacks consistent adherence to secure coding principles, potentially introducing common vulnerabilities.\n",
       "- **Inadequate Security Testing:** Evidence of dedicated security testing procedures and tools is absent.\n",
       "\n",
       "### 4. KEY RECOMMENDATIONS\n",
       "\n",
       "- **Prioritize Input Validation:** Implement rigorous input validation for all user-supplied data, leveraging Pydantic's capabilities and custom validation logic.\n",
       "- **Enforce Consistent Authentication & Authorization:** Utilize a centralized authentication and authorization mechanism (e.g., OAuth2, JWT) for all endpoints, including WebSockets.\n",
       "- **Secure File Upload Handling:** Enforce strict file type validation, size limits, and content sanitization for uploads. Store uploaded files in secure, non-public locations.\n",
       "- **Address Open Redirects:** Validate all redirect URLs against a whitelist of trusted domains.\n",
       "- **Harden Deployment Practices:**\n",
       "  - **Document and enforce the use of TLS for all connections.**\n",
       "  - **Implement security headers (e.g., Content-Security-Policy, X-Frame-Options).**\n",
       "  - **Provide guidance on secrets management for production environments.**\n",
       "- **Integrate Security Testing:** Incorporate security testing tools (e.g., SAST, DAST) and practices into the development lifecycle.\n",
       "- **Adopt a Secure Coding Standard:** Enforce a secure coding standard (e.g., OWASP ASVS) to minimize common vulnerabilities.\n",
       "\n",
       "**Addressing these recommendations will significantly enhance the security posture of the FastAPI application and reduce the risk of exploitation.**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Report file location: ./security_analysis_fastapi.git.md\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "def display_analysis_report(report_path: str):\n",
    "    \"\"\"Display the security analysis report in the notebook.\"\"\"\n",
    "    try:\n",
    "        report_content = Path(report_path).read_text(encoding='utf-8')\n",
    "        display(Markdown(report_content))\n",
    "        print(f\"\\nReport file location: {report_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading report: {str(e)}\")\n",
    "        \n",
    "display_analysis_report(report_path=\"./security_analysis_fastapi.git.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This notebook demonstrates the powerful capabilities of combining Large Language Models (specifically Gemini 1.5 Pro) with traditional security analysis approaches for comprehensive codebase security assessment. Through our analysis of FastAPI, we've shown how the `ComprehensiveSecurityAnalyzer` can effectively process and analyze diverse file types, detect potential security vulnerabilities, and provide actionable insights. The tool's ability to understand context across multiple files and identify cross-cutting security patterns makes it a valuable addition to existing security workflows.\n",
    "While LLM-based analysis should not replace traditional security tools and human expertise, it serves as a powerful complementary tool that can:\n",
    "\n",
    "- Rapidly process large codebases\n",
    "- Identify non-obvious security patterns\n",
    "- Provide context-aware recommendations\n",
    "- Help prioritize security concerns\n",
    "- Support security teams in making informed decisions\n",
    "\n",
    "Future improvements could include integration with vulnerability databases, custom security rule definitions, CI/CD pipeline integration, and expanded support for additional programming languages. As LLM capabilities continue to evolve, tools like this will become increasingly valuable for maintaining robust security practices in modern software development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
