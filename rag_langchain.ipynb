{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(\n",
    "    model=\"llama3.2:latest\",\n",
    "    request_timeout=300.0,\n",
    "    additional_kwargs={\"num_ctx\": 16384, \"num_predict\": -1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pk/d4txrj4d1hdb9jxkz_prd3fw0000gn/T/ipykernel_88923/2088320332.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embed_model = HuggingFaceEmbeddings(\n",
      "/Users/lunit_haseebraja/Developer/personal/projects/PaperGist/.venv/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    show_progress=True,\n",
    "    model_kwargs={\"device\": \"mps\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "[-0.0032757290173321962, -0.011690833605825901, 0.04155922308564186, -0.03814816474914551, 0.024183081462979317]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings = embed_model.embed_query(text=\"Hello world!\")\n",
    "print(len(embeddings))\n",
    "print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(file_path=\"./data/llama2.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def create_academic_text_splitter():\n",
    "    \"\"\"\n",
    "    Creates a text splitter optimized for academic papers with appropriate\n",
    "    chunk sizes and overlap to maintain context and section coherence.\n",
    "    \"\"\"\n",
    "    return RecursiveCharacterTextSplitter(\n",
    "        # Larger chunk size to keep more context together\n",
    "        chunk_size=3000,\n",
    "        # Significant overlap to maintain context across chunks\n",
    "        chunk_overlap=400,\n",
    "        # Common section headers in academic papers\n",
    "        separators=[\"\\n## \", \"\\n### \", \"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        # Keep sentences together\n",
    "        keep_separator=True,\n",
    "        # Merge smaller chunks\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.86it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.21it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize vector database and add nodes to it\n",
    "\n",
    "# from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain.indexes.vectorstore import VectorstoreIndexCreator\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "# Define host and port\n",
    "host = \"localhost\"\n",
    "port = \"6333\"\n",
    "\n",
    "# Create index with VectorStoreIndexCreator\n",
    "index_creator = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=QdrantVectorStore,\n",
    "    embedding=embed_model,\n",
    "    text_splitter=create_academic_text_splitter(),\n",
    "    vectorstore_kwargs={\n",
    "        \"collection_name\": \"rag_demo_collection\",\n",
    "        # \"location\": \":memory:\"\n",
    "        \"url\": f\"http://{host}:{port}\"  # Constructed from host and port\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create index\n",
    "index = index_creator.from_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "def create_system_prompt():\n",
    "    \"\"\"\n",
    "    Creates a system prompt template for paper summarization.\n",
    "    \"\"\"\n",
    "    return ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                (\n",
    "                    \"You are a world-class research scientist and expert in Large Language Models. You can break down complex ideas into digestable pieces and fetch key points from long research documents.\"\n",
    "                    \n",
    "                    \"Context information is below.\\n\"\n",
    "                    \"---------------------\\n\"\n",
    "                    \"{context}\\n\"\n",
    "                    \"---------------------\\n\"\n",
    "                    \n",
    "                    \"Using the context information, provide an accurate response to the user query.\\n\"\n",
    "                    \"Questions: {query}\\n\"\n",
    "                ),\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(query: str, top_docs=5):\n",
    "    \"\"\"\n",
    "    Get response using RAG pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    docs = index.vectorstore.similarity_search(query, k=top_docs)\n",
    "    \n",
    "    # Create the prompt and chain\n",
    "    prompt = create_system_prompt()\n",
    "    document_chain = create_stuff_documents_chain(\n",
    "        llm=llm,\n",
    "        prompt=prompt,\n",
    "    )\n",
    "\n",
    "    # Define the prompt variables\n",
    "    prompt_vars = {\n",
    "        \"context\": docs,\n",
    "        \"query\": query,\n",
    "    }\n",
    "\n",
    "    # Generate response using retrieved documents\n",
    "    for token in document_chain.stream(prompt_vars):\n",
    "        print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Detailed Analysis of the Research Paper on Large Language Models\n",
      "\n",
      "## Introduction\n",
      "\n",
      "The provided research paper focuses on Supervised Fine-Tuning (SFT) for large language models. The authors aim to improve the quality of the model by fine-tuning it with high-quality annotation data.\n",
      "\n",
      "### Key Takeaways\n",
      "\n",
      "* SFT is a crucial step in improving the performance of large language models.\n",
      "* High-quality annotation data is essential for achieving good results.\n",
      "* The authors implemented a quality assurance process to ensure that only high-quality annotations are used.\n",
      "\n",
      "## Getting Started\n",
      "\n",
      "The paper begins by discussing the importance of collecting high-quality SFT data. The authors highlight that third-party SFT data is available, but it often lacks diversity and quality.\n",
      "\n",
      "### Key Points\n",
      "\n",
      "* The authors started by collecting publicly available instruction tuning data (Chung et al., 2022) and used this data to bootstrap their SFT stage.\n",
      "* Quality is all you need: the authors focused on collecting high-quality SFT data, as illustrated in Table 5.\n",
      "* Third-party SFT data is available from many sources, but it often has insufficient diversity and quality.\n",
      "\n",
      "## Quality Assurance Process\n",
      "\n",
      "The authors implemented a quality assurance process to ensure that only high-quality annotations are used. This process involves:\n",
      "\n",
      "### Key Steps\n",
      "\n",
      "* A team of highly skilled content managers manually reviewed the annotations and approved the ones that would be used.\n",
      "* Reviewers were asked to approve annotations that:\n",
      "\t+ Were consistent with the dialogue history\n",
      "\t+ Followed instructions in the prompt\n",
      "\t+ Were free of grammatical, spelling, and other writing errors\n",
      "\t+ Did not fall into any of the categories described in Section A.5.2\n",
      "\n",
      "### Technical Details\n",
      "\n",
      "* Reviewers were allowed to edit annotations that needed small changes to be approved.\n",
      "* If an annotation could not be approved without major changes, reviewers were asked to reject it and write feedback necessary to improve it.\n",
      "\n",
      "## Annotator Selection\n",
      "\n",
      "The authors conducted a multi-step assessment process to select annotators for their data collection tasks. This process includes:\n",
      "\n",
      "### Key Tests\n",
      "\n",
      "* The first test consists of 3 sections:\n",
      "\t+ Grammar testing\n",
      "\t+ Reading comprehension testing\n",
      "\t+ Writing style testing\n",
      "* Each section is timed, and the test should take a total of 50 minutes to complete.\n",
      "* Candidates must score 90% on part I to continue on to parts II and III, and an average score of 4 on part II and III to pass the test.\n",
      "* The second test consists of:\n",
      "\t+ Sensitive topics alignment\n",
      "\t+ Answer ranking\n",
      "\t+ Two examples of answer writing\n",
      "* To pass the test, annotators needed to agree with the authors' criteria on 80% of the answers, and pass the written examples with a score of 4 out of 5.\n",
      "\n",
      "## Results\n",
      "\n",
      "The authors found that:\n",
      "\n",
      "### Key Findings\n",
      "\n",
      "* A limited set of clean instruction-tuning data can be sufficient to reach a high level of quality.\n",
      "* SFT annotations in the order of tens of thousands was enough to achieve a high-quality result.\n",
      "* The authors collected a total of 27,540 annotations and stopped annotating after this number.\n",
      "* They observed that different annotation platforms and vendors can result in markedly different downstream model performance.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "The authors' research provides valuable insights into the importance of high-quality annotation data for improving large language models. By implementing a quality assurance process and selecting annotators based on specific criteria, they were able to achieve good results. This paper highlights the need for researchers to carefully evaluate their annotation data and selection processes to ensure that only high-quality annotations are used.\n",
      "\n",
      "### Future Directions\n",
      "\n",
      "* Further research is needed to explore the effects of different annotation platforms and vendors on downstream model performance.\n",
      "* The authors' quality assurance process can be adapted and improved upon to ensure consistency across different datasets and models."
     ]
    }
   ],
   "source": [
    "instruction = \"Analyze the provided research paper with extreme attention to detail and provide a detailed analysis of the paper. Include all the key details from each sections of the paper.\"\n",
    "data_format = \"Organize your nice and organized markdown format. Use headings and subheadings to make it easy to read and understand. Use bullet-points wherever necessary.\"\n",
    "audience = \"While this is for busy researchers, provide complete technical depth. Do not summarize or simplify technical details.\"\n",
    "tone = \"The tone should be professional and clear.\"\n",
    "\n",
    "query_str = (\n",
    "    f\"{instruction}\\n\\n\" f\"{data_format}\\n\\n\" f\"{audience}\\n\\n\" f\"{tone}\\n\\n\"\n",
    ")\n",
    "\n",
    "get_response(query=query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fine-tuning process of Llama 2 involves several stages and techniques:\n",
      "\n",
      "1. **Supervised Fine-Tuning**: The initial version of Llama 2-Chat is created through supervised fine-tuning, where the model is trained on a dataset specifically designed for chat-based conversations.\n",
      "\n",
      "2. **Reward Modeling**: After the initial fine-tuning, the model undergoes iterative reward modeling using techniques such as rejection sampling and Proximal Policy Optimization (PPO).\n",
      "\n",
      "3. **Reinforcement Learning with Human Feedback (RLHF)**: The model is further refined through RLHF methodologies, which involve aligning the model's behavior with human feedback.\n",
      "\n",
      "4. **Ghost Attention (GAtt)**: A new technique called Ghost Attention is introduced to control dialogue flow over multiple turns.\n",
      "\n",
      "The fine-tuning process involves several iterations, and the accumulation of reward modeling data in parallel with model enhancements is crucial to ensure that the reward models remain within distribution."
     ]
    }
   ],
   "source": [
    "get_response(query=\"Summarize the fine-tuning process of llama2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
